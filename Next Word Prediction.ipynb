{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8c8b2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82179b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=requests.get(\"http://www.gutenberg.org/cache/epub/5200/pg5200.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d489204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Metamorphosis, by Franz Kafka\\r\\nTranslated by David Wyllie.\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\r\\nre-use it under the terms of the Project Gutenberg License included\\r\\nwith this eBook or online at www.gutenberg.org\\r\\n\\r\\n** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **\\r\\n**     Please follow the copyright guidelines in this file.     **\\r\\n\\r\\n\\r\\nTitle: Metamorphosis\\r\\n\\r\\nAuthor: Franz Kafka\\r\\n\\r\\nTranslator: David Wyllie\\r\\n\\r\\nRelease Date: August 16, 2005 [EBook #5200]\\r\\nFirst posted: May 13, 2002\\r\\nLast updated: May 20, 2012\\r\\n\\r\\nLanguage: English\\r\\n\\r\\n\\r\\n*** START OF THIS PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nCopyright (C) 2002 David Wyllie.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n  Metamorphosis\\r\\n  Franz Kafka\\r\\n\\r\\nTranslated by David Wyllie\\r\\n\\r\\n\\r\\n\\r\\nI\\r\\n\\r\\n\\r\\nOne morning, when Gregor Samsa woke from troubled dreams, he found\\r\\nhimself transformed in his bed into a horrible vermin.  He lay on\\r\\nhis armour-like back, and if he lifted his head a little he could\\r\\nsee his brown belly, slightly domed and divided by arches into stiff\\r\\nsections.  The bedding was hardly able to cover it and seemed ready\\r\\nto slide off any moment.  His many legs, pitifully thin compared\\r\\nwith the size of the rest of him, waved about helplessly as he\\r\\nlooked.\\r\\n\\r\\n\"What\\'s happened to me?\" he thought.  It wasn\\'t a dream.  His room,\\r\\na proper human room although a little too small, lay peacefully\\r\\nbetwee'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text[:1500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80858479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Metamorphosis, by Franz Kafka\\r'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = response.text.split('\\n')\n",
    "data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9741da1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'away from the bed, bend down with the load and then be patient and\\r'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[253:]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c41528ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2110"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30de489f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'away from the bed, bend down with the load and then be patient and\\r careful as he swang over onto the floor, where, hopefully, the\\r little legs would find a use.  Should he really call for help\\r though, even apart from the fact that all the doors were locked?\\r Despite all the difficulty he was in, he could not suppress a smile\\r at this thought.\\r \\r After a while he had already moved so far across that it would have\\r been hard for him to keep his balance if he rocked too hard.  The\\r time was now ten past seven and he would have to make a final\\r decision very soon.  Then there was a ring at the door of the flat.\\r \"That\\'ll be someone from work\", he said to himself, and froze very\\r still, although his little legs only became all the more lively as\\r they danced around.  For a moment everything remained quiet.\\r \"They\\'re not opening the door\", Gregor said to himself, caught in\\r some nonsensical hope.  But then of course, the maid\\'s firm steps\\r went to the door as ever and opened it.  Gregor on'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \" \".join(data)\n",
    "data[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e25a0aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['away', 'from', 'the', 'bed', 'bend', 'down', 'with', 'the', 'load', 'and', 'then', 'be', 'patient', 'and', 'careful', 'as', 'he', 'swang', 'over', 'onto', 'the', 'floor', 'where', 'hopefully', 'the', 'little', 'legs', 'would', 'find', 'a', 'use', 'should', 'he', 'really', 'call', 'for', 'help', 'though', 'even', 'apart', 'from', 'the', 'fact', 'that', 'all', 'the', 'doors', 'were', 'locked', 'despite']\n"
     ]
    }
   ],
   "source": [
    "def clean_text(doc):\n",
    " tokens = doc.split()\n",
    " table = str.maketrans('', '', string.punctuation)\n",
    " tokens = [w.translate(table) for w in tokens]\n",
    " tokens = [word for word in tokens if word.isalpha()]\n",
    " tokens = [word.lower() for word in tokens]\n",
    " return tokens\n",
    "tokens = clean_text(data)\n",
    "print(tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30895ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22607"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb7698a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22556\n"
     ]
    }
   ],
   "source": [
    "length = 50 + 1\n",
    "lines = []\n",
    "for i in range(length, len(tokens)):\n",
    " seq = tokens[i-length:i]\n",
    " line = ' '.join(seq)\n",
    " lines.append(line)\n",
    " if i > 200000:\n",
    "        break\n",
    "\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1ee912",
   "metadata": {},
   "source": [
    "# Build LSTM Model and Prepare X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06bff5e",
   "metadata": {},
   "source": [
    "#### import all the necessary libraries used to pre-process the data and create the layers of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d96c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5453ff1c",
   "metadata": {},
   "source": [
    "##### We are going to create a unique numerical token for each unique word in the dataset.fit_on_texts() updates internal vocabulary based on a list of texts. texts_to_sequences() transforms each text in texts to a sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a0aa777",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4bb064",
   "metadata": {},
   "source": [
    "##### sequences containes a list of integer values created by tokenizer. Each line in sequences has 51 words. Now we will split each line such that the first 50 words are in X and the last word is in y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5e71638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 103,   29,    1,  245, 2883,   98,   14,    1, 1435,    3,   48,\n",
       "         30,  618,    3,  756,   13,    6, 1434,  107,  165,    1,  149,\n",
       "         86, 2880,    1,   78,  225,   21,  530,   12,  156,  193,    6,\n",
       "        142,  754,   17,  180,  116,   49, 1433,   29,    1,  753,   11,\n",
       "         26,    1,  455,   58,  617,  329])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:, :-1], sequences[:,-1]\n",
    "X[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3664a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b934451",
   "metadata": {},
   "source": [
    "##### to_categorical() converts a class vector (integers) to binary class matrix. num_classes is the total number of classes which is vocab_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc594aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef8ecbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = X.shape[1]\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90df7b6e",
   "metadata": {},
   "source": [
    "# LSTM Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a071177",
   "metadata": {},
   "source": [
    "##### A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93ace825",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9f7eb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            144250    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2885)              291385    \n",
      "=================================================================\n",
      "Total params: 586,535\n",
      "Trainable params: 586,535\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93d1622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145584c6",
   "metadata": {},
   "source": [
    "##### After compiling the model we will now train the model using model.fit() on the training dataset. We will use 100 epochs to train the model. An epoch is an iteration over the entire x and y data provided. batch_size is the number of samples per gradient update i.e. the weights will be updates after 256 training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a694ec7e",
   "metadata": {},
   "source": [
    "##### We are now going to generate words using the model. For this we need a set of 50 words to predict the 51st word. So we are taking a random line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e37215a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "89/89 [==============================] - 45s 466ms/step - loss: 6.6415 - accuracy: 0.0531\n",
      "Epoch 2/100\n",
      "89/89 [==============================] - 41s 459ms/step - loss: 6.1876 - accuracy: 0.0540\n",
      "Epoch 3/100\n",
      "89/89 [==============================] - 44s 495ms/step - loss: 6.1617 - accuracy: 0.0540\n",
      "Epoch 4/100\n",
      "89/89 [==============================] - 45s 506ms/step - loss: 6.0496 - accuracy: 0.0540\n",
      "Epoch 5/100\n",
      "89/89 [==============================] - 42s 469ms/step - loss: 5.9166 - accuracy: 0.0565\n",
      "Epoch 6/100\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 5.7821 - accuracy: 0.0606\n",
      "Epoch 7/100\n",
      "89/89 [==============================] - 53s 595ms/step - loss: 5.6837 - accuracy: 0.0674\n",
      "Epoch 8/100\n",
      "89/89 [==============================] - 46s 513ms/step - loss: 5.6051 - accuracy: 0.0728\n",
      "Epoch 9/100\n",
      "89/89 [==============================] - 41s 457ms/step - loss: 5.5427 - accuracy: 0.0782\n",
      "Epoch 10/100\n",
      "89/89 [==============================] - 41s 458ms/step - loss: 5.4835 - accuracy: 0.0818\n",
      "Epoch 11/100\n",
      "89/89 [==============================] - 42s 466ms/step - loss: 5.4249 - accuracy: 0.0876\n",
      "Epoch 12/100\n",
      "89/89 [==============================] - 43s 480ms/step - loss: 5.3603 - accuracy: 0.0898\n",
      "Epoch 13/100\n",
      "89/89 [==============================] - 42s 471ms/step - loss: 5.2945 - accuracy: 0.0969\n",
      "Epoch 14/100\n",
      "89/89 [==============================] - 40s 448ms/step - loss: 5.2311 - accuracy: 0.1013\n",
      "Epoch 15/100\n",
      "89/89 [==============================] - 41s 456ms/step - loss: 5.1778 - accuracy: 0.1060\n",
      "Epoch 16/100\n",
      "89/89 [==============================] - 41s 458ms/step - loss: 5.1183 - accuracy: 0.1108\n",
      "Epoch 17/100\n",
      "89/89 [==============================] - 40s 454ms/step - loss: 5.0589 - accuracy: 0.1170\n",
      "Epoch 18/100\n",
      "89/89 [==============================] - 40s 451ms/step - loss: 5.0888 - accuracy: 0.1146\n",
      "Epoch 19/100\n",
      "89/89 [==============================] - 41s 459ms/step - loss: 5.1140 - accuracy: 0.1162\n",
      "Epoch 20/100\n",
      "89/89 [==============================] - 41s 459ms/step - loss: 4.9669 - accuracy: 0.1236\n",
      "Epoch 21/100\n",
      "89/89 [==============================] - 41s 455ms/step - loss: 4.8830 - accuracy: 0.1287\n",
      "Epoch 22/100\n",
      "89/89 [==============================] - 41s 459ms/step - loss: 4.8055 - accuracy: 0.1322\n",
      "Epoch 23/100\n",
      "89/89 [==============================] - 40s 445ms/step - loss: 4.7168 - accuracy: 0.1405\n",
      "Epoch 24/100\n",
      "89/89 [==============================] - 40s 450ms/step - loss: 4.6409 - accuracy: 0.1453\n",
      "Epoch 25/100\n",
      "89/89 [==============================] - 41s 457ms/step - loss: 4.5743 - accuracy: 0.1487\n",
      "Epoch 26/100\n",
      "89/89 [==============================] - 40s 449ms/step - loss: 4.5095 - accuracy: 0.1535\n",
      "Epoch 27/100\n",
      "89/89 [==============================] - 41s 459ms/step - loss: 4.4536 - accuracy: 0.1564\n",
      "Epoch 28/100\n",
      "89/89 [==============================] - 40s 452ms/step - loss: 4.3980 - accuracy: 0.1608\n",
      "Epoch 29/100\n",
      "89/89 [==============================] - 41s 457ms/step - loss: 4.3419 - accuracy: 0.1635\n",
      "Epoch 30/100\n",
      "89/89 [==============================] - 40s 445ms/step - loss: 4.2865 - accuracy: 0.1681\n",
      "Epoch 31/100\n",
      "89/89 [==============================] - 40s 450ms/step - loss: 4.2351 - accuracy: 0.1677\n",
      "Epoch 32/100\n",
      "89/89 [==============================] - 40s 448ms/step - loss: 4.1819 - accuracy: 0.1717\n",
      "Epoch 33/100\n",
      "89/89 [==============================] - 41s 458ms/step - loss: 4.1351 - accuracy: 0.1751\n",
      "Epoch 34/100\n",
      "89/89 [==============================] - 40s 450ms/step - loss: 4.0859 - accuracy: 0.1768\n",
      "Epoch 35/100\n",
      "89/89 [==============================] - 40s 444ms/step - loss: 4.0375 - accuracy: 0.1818\n",
      "Epoch 36/100\n",
      "89/89 [==============================] - 41s 459ms/step - loss: 3.9909 - accuracy: 0.1851\n",
      "Epoch 37/100\n",
      "89/89 [==============================] - 40s 451ms/step - loss: 3.9477 - accuracy: 0.1877\n",
      "Epoch 38/100\n",
      "89/89 [==============================] - 41s 463ms/step - loss: 3.8992 - accuracy: 0.1918\n",
      "Epoch 39/100\n",
      "89/89 [==============================] - 40s 454ms/step - loss: 3.8556 - accuracy: 0.1939\n",
      "Epoch 40/100\n",
      "89/89 [==============================] - 40s 452ms/step - loss: 3.8083 - accuracy: 0.1998\n",
      "Epoch 41/100\n",
      "89/89 [==============================] - 40s 455ms/step - loss: 3.7618 - accuracy: 0.2045\n",
      "Epoch 42/100\n",
      "89/89 [==============================] - 40s 445ms/step - loss: 3.7215 - accuracy: 0.2079\n",
      "Epoch 43/100\n",
      "89/89 [==============================] - 41s 459ms/step - loss: 3.6769 - accuracy: 0.2115\n",
      "Epoch 44/100\n",
      "89/89 [==============================] - 41s 458ms/step - loss: 3.6303 - accuracy: 0.2160\n",
      "Epoch 45/100\n",
      "89/89 [==============================] - 41s 461ms/step - loss: 3.5873 - accuracy: 0.2215\n",
      "Epoch 46/100\n",
      "89/89 [==============================] - 41s 456ms/step - loss: 3.5451 - accuracy: 0.2282\n",
      "Epoch 47/100\n",
      "89/89 [==============================] - 41s 460ms/step - loss: 3.5033 - accuracy: 0.2325\n",
      "Epoch 48/100\n",
      "89/89 [==============================] - 40s 448ms/step - loss: 3.4583 - accuracy: 0.2388\n",
      "Epoch 49/100\n",
      "89/89 [==============================] - 40s 445ms/step - loss: 3.4139 - accuracy: 0.2442\n",
      "Epoch 50/100\n",
      "89/89 [==============================] - 40s 451ms/step - loss: 3.3703 - accuracy: 0.2510\n",
      "Epoch 51/100\n",
      "89/89 [==============================] - 40s 453ms/step - loss: 3.3321 - accuracy: 0.2548\n",
      "Epoch 52/100\n",
      "89/89 [==============================] - 40s 452ms/step - loss: 3.2884 - accuracy: 0.2627\n",
      "Epoch 53/100\n",
      "89/89 [==============================] - 40s 451ms/step - loss: 3.2466 - accuracy: 0.2694\n",
      "Epoch 54/100\n",
      "89/89 [==============================] - 40s 453ms/step - loss: 3.2052 - accuracy: 0.2743\n",
      "Epoch 55/100\n",
      "89/89 [==============================] - 41s 459ms/step - loss: 3.1688 - accuracy: 0.2814\n",
      "Epoch 56/100\n",
      "89/89 [==============================] - 41s 460ms/step - loss: 3.1276 - accuracy: 0.2860\n",
      "Epoch 57/100\n",
      "89/89 [==============================] - 40s 453ms/step - loss: 3.0883 - accuracy: 0.2940\n",
      "Epoch 58/100\n",
      "89/89 [==============================] - 40s 453ms/step - loss: 3.0489 - accuracy: 0.3001\n",
      "Epoch 59/100\n",
      "89/89 [==============================] - 41s 458ms/step - loss: 3.0105 - accuracy: 0.3076\n",
      "Epoch 60/100\n",
      "89/89 [==============================] - 41s 458ms/step - loss: 2.9666 - accuracy: 0.3136\n",
      "Epoch 61/100\n",
      "89/89 [==============================] - 41s 460ms/step - loss: 2.9331 - accuracy: 0.3212\n",
      "Epoch 62/100\n",
      "89/89 [==============================] - 40s 447ms/step - loss: 2.8984 - accuracy: 0.3257\n",
      "Epoch 63/100\n",
      "89/89 [==============================] - 40s 455ms/step - loss: 2.8620 - accuracy: 0.3310\n",
      "Epoch 64/100\n",
      "89/89 [==============================] - 41s 460ms/step - loss: 2.8221 - accuracy: 0.3388\n",
      "Epoch 65/100\n",
      "89/89 [==============================] - 40s 454ms/step - loss: 2.7906 - accuracy: 0.3471\n",
      "Epoch 66/100\n",
      "89/89 [==============================] - 40s 447ms/step - loss: 2.7555 - accuracy: 0.3533\n",
      "Epoch 67/100\n",
      "89/89 [==============================] - 41s 462ms/step - loss: 2.7137 - accuracy: 0.3631\n",
      "Epoch 68/100\n",
      "89/89 [==============================] - 41s 461ms/step - loss: 2.6868 - accuracy: 0.3674\n",
      "Epoch 69/100\n",
      "89/89 [==============================] - 40s 451ms/step - loss: 2.6583 - accuracy: 0.3685\n",
      "Epoch 70/100\n",
      "89/89 [==============================] - 40s 448ms/step - loss: 2.6224 - accuracy: 0.3779\n",
      "Epoch 71/100\n",
      "89/89 [==============================] - 41s 455ms/step - loss: 2.6126 - accuracy: 0.3819\n",
      "Epoch 72/100\n",
      "89/89 [==============================] - 41s 456ms/step - loss: 2.5676 - accuracy: 0.3899\n",
      "Epoch 73/100\n",
      "89/89 [==============================] - 41s 456ms/step - loss: 2.5276 - accuracy: 0.3973\n",
      "Epoch 74/100\n",
      "89/89 [==============================] - 40s 455ms/step - loss: 2.4898 - accuracy: 0.4046\n",
      "Epoch 75/100\n",
      "89/89 [==============================] - 40s 447ms/step - loss: 2.4586 - accuracy: 0.4090\n",
      "Epoch 76/100\n",
      "89/89 [==============================] - 41s 456ms/step - loss: 2.4335 - accuracy: 0.4166\n",
      "Epoch 77/100\n",
      "89/89 [==============================] - 40s 452ms/step - loss: 2.3976 - accuracy: 0.4241\n",
      "Epoch 78/100\n",
      "89/89 [==============================] - 41s 456ms/step - loss: 2.3625 - accuracy: 0.4292\n",
      "Epoch 79/100\n",
      "89/89 [==============================] - 41s 458ms/step - loss: 2.3441 - accuracy: 0.4322\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 41s 461ms/step - loss: 2.3113 - accuracy: 0.4416\n",
      "Epoch 81/100\n",
      "89/89 [==============================] - 41s 461ms/step - loss: 2.2765 - accuracy: 0.4485\n",
      "Epoch 82/100\n",
      "89/89 [==============================] - 41s 462ms/step - loss: 2.2487 - accuracy: 0.4542\n",
      "Epoch 83/100\n",
      "89/89 [==============================] - 40s 451ms/step - loss: 2.2233 - accuracy: 0.4588\n",
      "Epoch 84/100\n",
      "89/89 [==============================] - 39s 443ms/step - loss: 2.1975 - accuracy: 0.4665\n",
      "Epoch 85/100\n",
      "89/89 [==============================] - 40s 452ms/step - loss: 2.1710 - accuracy: 0.4702\n",
      "Epoch 86/100\n",
      "89/89 [==============================] - 41s 455ms/step - loss: 2.1379 - accuracy: 0.4787\n",
      "Epoch 87/100\n",
      "89/89 [==============================] - 44s 498ms/step - loss: 2.1049 - accuracy: 0.4826\n",
      "Epoch 88/100\n",
      "89/89 [==============================] - 43s 480ms/step - loss: 2.0824 - accuracy: 0.4896\n",
      "Epoch 89/100\n",
      "89/89 [==============================] - 50s 560ms/step - loss: 2.0534 - accuracy: 0.4947\n",
      "Epoch 90/100\n",
      "89/89 [==============================] - 51s 569ms/step - loss: 2.0291 - accuracy: 0.5021\n",
      "Epoch 91/100\n",
      "89/89 [==============================] - 51s 575ms/step - loss: 1.9964 - accuracy: 0.5079\n",
      "Epoch 92/100\n",
      "89/89 [==============================] - 49s 553ms/step - loss: 1.9744 - accuracy: 0.5125\n",
      "Epoch 93/100\n",
      "89/89 [==============================] - 54s 603ms/step - loss: 1.9564 - accuracy: 0.5162\n",
      "Epoch 94/100\n",
      "89/89 [==============================] - 52s 583ms/step - loss: 1.9349 - accuracy: 0.5210\n",
      "Epoch 95/100\n",
      "89/89 [==============================] - 47s 526ms/step - loss: 1.9049 - accuracy: 0.5278\n",
      "Epoch 96/100\n",
      "89/89 [==============================] - 46s 516ms/step - loss: 1.8711 - accuracy: 0.5382\n",
      "Epoch 97/100\n",
      "89/89 [==============================] - 42s 477ms/step - loss: 1.8530 - accuracy: 0.5374\n",
      "Epoch 98/100\n",
      "89/89 [==============================] - 43s 489ms/step - loss: 1.8265 - accuracy: 0.5470\n",
      "Epoch 99/100\n",
      "89/89 [==============================] - 42s 470ms/step - loss: 1.8051 - accuracy: 0.5486\n",
      "Epoch 100/100\n",
      "89/89 [==============================] - 44s 490ms/step - loss: 1.7796 - accuracy: 0.5573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24a9db10250>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size = 256, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0895e1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'condition seemed serious enough to remind even his father that gregor despite his current sad and revolting form was a family member who could not be treated as an enemy on the contrary as a family there was a duty to swallow any revulsion for him and to be patient just'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text=lines[12343]\n",
    "seed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a7d28b",
   "metadata": {},
   "source": [
    "###### generate_text_seq() generates n_words number of words after the given seed_text. We are going to pre-process the seed_text before predicting. We are going to encode the seed_text using the same encoding used for encoding the training data. Then we are going to convert the seed_textto 50 words by using pad_sequences(). Now we will predict using model.predict_classes(). After that we will search the word in tokenizer using the index in y_predict. Finally we will append the predicted word to seed_text and text and repeat the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd241e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_seq(model, tokenizer, text_seq_length, seed_text, n_words):\n",
    "    text = []\n",
    "    \n",
    " \n",
    "    for _ in range(n_words):\n",
    "        \n",
    "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        encoded = pad_sequences([encoded], maxlen = text_seq_length, truncating='pre')\n",
    "        y_predict = model.predict_classes(encoded)\n",
    "        predicted_word = ''\n",
    "        \n",
    "        for word, index in tokenizer.word_index.items(): \n",
    "            if index == y_predict: \n",
    "                predicted_word = word \n",
    "                break\n",
    "                \n",
    "        seed_text = seed_text + ' ' + predicted_word\n",
    "        \n",
    "        text.append(predicted_word)\n",
    "    return ' '.join(text)\n",
    "       \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2200447",
   "metadata": {},
   "source": [
    "##### We can see that the next 100 words are predicted by the model for the seed_text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20444aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chance chance chance added sleeve covering goes goes abandoned abandoned shock shock shock moment solid solid solid awful awful example example example family dark dark expected expected moment moment reply balls balls balls problem imagined imagined imagined imagined imagined rage rage nearer explained explained nearer explained explained nearer explained explained total limitation kiss harmed harmed forgotten forgotten intention intention intention intention repelled repelled repelled repelled shock shock downloading he slight slight attached shoulders shoulders shoulders explained explained explained explained explained chosen chosen chosen run run run run run run run fancy fancy fancy salts salts salts salts contact contact load'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_seq(model, tokenizer, seq_length, seed_text, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db766283",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
